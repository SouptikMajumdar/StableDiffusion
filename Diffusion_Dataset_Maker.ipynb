{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SouptikMajumdar/StableDiffusion/blob/main/Diffusion_Dataset_Maker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmCPmqFL6hCQ"
      },
      "source": [
        "# üìä Dataset Maker by Hollowstrawberry\n",
        "\n",
        "This is based on the work of [Kohya-ss](https://github.com/kohya-ss/sd-scripts) and [Linaqruf](https://colab.research.google.com/github/Linaqruf/kohya-trainer/blob/main/kohya-LoRA-dreambooth.ipynb). Thank you!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2IcjZgMhT2h"
      },
      "source": [
        "### ‚≠ï Disclaimer\n",
        "The purpose of this document is to research bleeding-edge technologies in the field of machine learning inference.  \n",
        "Please read and follow the [Google Colab guidelines](https://research.google.com/colaboratory/faq.html) and its [Terms of Service](https://research.google.com/colaboratory/tos_v3.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rdgF2AWLS2h"
      },
      "source": [
        "| |GitHub|üá¨üáß English|üá™üá∏ Spanish|\n",
        "|:--|:-:|:-:|:-:|\n",
        "| üè† **Homepage** | [![GitHub](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/github.svg)](https://github.com/hollowstrawberry/kohya-colab) | | |\n",
        "| üìä **Dataset Maker** | [![GitHub](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/github.svg)](https://github.com/hollowstrawberry/kohya-colab/blob/main/Dataset_Maker.ipynb) | [![Open in Colab](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/colab-badge.svg)](https://colab.research.google.com/github/hollowstrawberry/kohya-colab/blob/main/Dataset_Maker.ipynb) | [![Abrir en Colab](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/colab-badge-spanish.svg)](https://colab.research.google.com/github/hollowstrawberry/kohya-colab/blob/main/Spanish_Dataset_Maker.ipynb) |\n",
        "| ‚≠ê **Lora Trainer** | [![GitHub](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/github.svg)](https://github.com/hollowstrawberry/kohya-colab/blob/main/Lora_Trainer.ipynb) | [![Open in Colab](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/colab-badge.svg)](https://colab.research.google.com/github/hollowstrawberry/kohya-colab/blob/main/Lora_Trainer.ipynb) | [![Abrir en Colab](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/colab-badge-spanish.svg)](https://colab.research.google.com/github/hollowstrawberry/kohya-colab/blob/main/Spanish_Lora_Trainer.ipynb) |\n",
        "| üåü **XL Lora Trainer** | [![GitHub](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/github.svg)](https://github.com/hollowstrawberry/kohya-colab/blob/main/Lora_Trainer_XL.ipynb) | [![Open in Colab](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/colab-badge.svg)](https://colab.research.google.com/github/hollowstrawberry/kohya-colab/blob/main/Lora_Trainer_XL.ipynb) |  |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cBa7KdewQ4BU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a55c2b75-dbde-49ee-8f3d-6804ce0c8d9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Project lydia is ready!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from IPython import get_ipython\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "COLAB = True\n",
        "\n",
        "if COLAB:\n",
        "  from google.colab.output import clear as clear_output\n",
        "else:\n",
        "  from IPython.display import clear_output\n",
        "\n",
        "#@title ## üö© Start Here\n",
        "\n",
        "#@markdown ### 1Ô∏è‚É£ Setup\n",
        "#@markdown This cell will load some requirements and create the necessary folders in your Google Drive. <p>\n",
        "#@markdown Your project name can't contain spaces but it can contain a single / to make a subfolder in your dataset.\n",
        "project_name = \"lydia\" #@param {type:\"string\"}\n",
        "project_name = project_name.strip()\n",
        "#@markdown The folder structure doesn't matter and is purely for comfort. Make sure to always pick the same one. I like organizing by project.\n",
        "folder_structure = \"Organize by project (MyDrive/Loras/project_name/dataset)\" #@param [\"Organize by category (MyDrive/lora_training/datasets/project_name)\", \"Organize by project (MyDrive/Loras/project_name/dataset)\"]\n",
        "\n",
        "if not project_name or any(c in project_name for c in \" .()\\\"'\\\\\") or project_name.count(\"/\") > 1:\n",
        "  print(\"Please write a valid project_name.\")\n",
        "else:\n",
        "  if COLAB and not os.path.exists('/content/drive'):\n",
        "    from google.colab import drive\n",
        "    print(\"üìÇ Connecting to Google Drive...\")\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "  project_base = project_name if \"/\" not in project_name else project_name[:project_name.rfind(\"/\")]\n",
        "  project_subfolder = project_name if \"/\" not in project_name else project_name[project_name.rfind(\"/\")+1:]\n",
        "\n",
        "  root_dir = \"/content\" if COLAB else \"~/Loras\"\n",
        "  deps_dir = os.path.join(root_dir, \"deps\")\n",
        "\n",
        "  if \"/Loras\" in folder_structure:\n",
        "    main_dir      = os.path.join(root_dir, \"drive/MyDrive/Loras\") if COLAB else root_dir\n",
        "    config_folder = os.path.join(main_dir, project_base)\n",
        "    images_folder = os.path.join(main_dir, project_base, \"dataset\")\n",
        "    if \"/\" in project_name:\n",
        "      images_folder = os.path.join(images_folder, project_subfolder)\n",
        "  else:\n",
        "    main_dir      = os.path.join(root_dir, \"drive/MyDrive/lora_training\") if COLAB else root_dir\n",
        "    config_folder = os.path.join(main_dir, \"config\", project_name)\n",
        "    images_folder = os.path.join(main_dir, \"datasets\", project_name)\n",
        "\n",
        "  for dir in [main_dir, deps_dir, images_folder, config_folder]:\n",
        "    os.makedirs(dir, exist_ok=True)\n",
        "\n",
        "  print(f\"‚úÖ Project {project_name} is ready!\")\n",
        "  step1_installed_flag = True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "afu5dCKTV31E"
      },
      "outputs": [],
      "source": [
        "if \"step1_installed_flag\" not in globals():\n",
        "  raise Exception(\"Please run step 1 first!\")\n",
        "\n",
        "import json\n",
        "import time\n",
        "from urllib.request import urlopen, Request\n",
        "\n",
        "#@markdown ### 2Ô∏è‚É£ Scrape images from Gelbooru\n",
        "\n",
        "#@markdown We will grab images from the popular anime gallery [Gelbooru](https://gelbooru.com/). Images are sorted by tags, including poses, scenes, character traits, character names, artists, etc. <p>\n",
        "#@markdown * If you instead want to use your own images, upload them to your Google Drive's `Loras/project_name/dataset` folder.\n",
        "#@markdown * If you instead want to download screencaps of anime episodes, try [this other colab by another person](https://colab.research.google.com/drive/1oBSntB40BKzNmKceXUlkXzujzdQw-Ci7). It's more complicated though.\n",
        "\n",
        "#@markdown Up to 1000 images may be downloaded by this step in just one minute. Don't abuse it. <p>\n",
        "#@markdown Your target tags should include the relevant tags for your character/concept/artstyle, and exclude undesired tags (for example, explicit images may affect learning).\n",
        "#@markdown Separate words with underscores, separate tags with spaces, and use - to exclude a tag. You can also include a minimum score: `score:>10`\n",
        "tags = \"1girl -sex -bdsm -loli -greyscale -monochrome hatsune_miku\" #@param {type:\"string\"}\n",
        "##@markdown If an image is bigger than this resolution a smaller version will be downloaded instead.\n",
        "max_resolution = 3072 #param {type:\"slider\", min:1024, max:8196, step:1024}\n",
        "##@markdown Posts with a parent post are often minor variations of the same image.\n",
        "include_posts_with_parent = True #param {type:\"boolean\"}\n",
        "\n",
        "tags = tags.replace(\" \", \"+\")\\\n",
        "           .replace(\"(\", \"%28\")\\\n",
        "           .replace(\")\", \"%29\")\\\n",
        "           .replace(\":\", \"%3a\")\\\n",
        "           .replace(\"&\", \"%26\")\\\n",
        "\n",
        "url = \"https://gelbooru.com/index.php?page=dapi&json=1&s=post&q=index&limit=100&tags={}\".format(tags)\n",
        "user_agent = \"Mozilla/5.0 AppleWebKit/537.36 (KHTML, like Gecko; compatible; Googlebot/2.1; +http://www.google.com/bot.html) Chrome/93.0.4577.83 Safari/537.36\"\n",
        "limit = 100 # hardcoded by gelbooru\n",
        "total_limit = 1000 # you can edit this if you want but I wouldn't recommend it\n",
        "supported_types = (\".png\", \".jpg\", \".jpeg\")\n",
        "\n",
        "def ubuntu_deps():\n",
        "  print(\"üè≠ Installing dependencies...\\n\")\n",
        "  !apt -y install aria2\n",
        "  return not get_ipython().__dict__['user_ns']['_exit_code']\n",
        "\n",
        "if \"step2_installed_flag\" not in globals():\n",
        "  if ubuntu_deps():\n",
        "    clear_output()\n",
        "    step2_installed_flag = True\n",
        "  else:\n",
        "    print(\"‚ùå Error installing dependencies, attempting to continue anyway...\")\n",
        "\n",
        "def get_json(url):\n",
        "  with urlopen(Request(url, headers={\"User-Agent\": user_agent})) as page:\n",
        "    return json.load(page)\n",
        "\n",
        "def filter_images(data):\n",
        "  return [p[\"file_url\"] if p[\"width\"]*p[\"height\"] <= max_resolution**2 else p[\"sample_url\"]\n",
        "          for p in data[\"post\"]\n",
        "          if (p[\"parent_id\"] == 0 or include_posts_with_parent)\n",
        "          and p[\"file_url\"].lower().endswith(supported_types)]\n",
        "\n",
        "def download_images():\n",
        "  data = get_json(url)\n",
        "  count = data[\"@attributes\"][\"count\"]\n",
        "\n",
        "  if count == 0:\n",
        "    print(\"üì∑ No results found\")\n",
        "    return\n",
        "\n",
        "  print(f\"üéØ Found {count} results\")\n",
        "  test_url = \"https://gelbooru.com/index.php?page=post&s=list&tags={}\".format(tags)\n",
        "  display(Markdown(f\"[Click here to open in browser!]({test_url})\"))\n",
        "  print (f\"üîΩ Will download to {images_folder.replace('/content/drive/', '')} (A confirmation box should appear below, otherwise run this cell again)\")\n",
        "  inp = input(\"‚ùì Enter the word 'yes' if you want to proceed with the download: \")\n",
        "\n",
        "  if inp.lower().strip() != 'yes':\n",
        "    print(\"‚ùå Download cancelled\")\n",
        "    return\n",
        "\n",
        "  print(\"üì© Grabbing image list...\")\n",
        "\n",
        "  image_urls = set()\n",
        "  image_urls = image_urls.union(filter_images(data))\n",
        "  for i in range(total_limit // limit):\n",
        "    count -= limit\n",
        "    if count <= 0:\n",
        "      break\n",
        "    time.sleep(0.1)\n",
        "    image_urls = image_urls.union(filter_images(get_json(url+f\"&pid={i+1}\")))\n",
        "\n",
        "  scrape_file = os.path.join(config_folder, f\"scrape_{project_subfolder}.txt\")\n",
        "  with open(scrape_file, \"w\") as f:\n",
        "    f.write(\"\\n\".join(image_urls))\n",
        "\n",
        "  print(f\"üåê Saved links to {scrape_file}\\n\\nüîÅ Downloading images...\\n\")\n",
        "  old_img_count = len([f for f in os.listdir(images_folder) if f.lower().endswith(supported_types)])\n",
        "\n",
        "  os.chdir(images_folder)\n",
        "  !aria2c --console-log-level=warn -c -x 16 -k 1M -s 16 -i {scrape_file}\n",
        "\n",
        "  new_img_count = len([f for f in os.listdir(images_folder) if f.lower().endswith(supported_types)])\n",
        "  print(f\"\\n‚úÖ Downloaded {new_img_count - old_img_count} images.\")\n",
        "\n",
        "download_images()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "b218DEEMpwzB"
      },
      "outputs": [],
      "source": [
        "if \"step1_installed_flag\" not in globals():\n",
        "  raise Exception(\"Please run step 1 first!\")\n",
        "\n",
        "#@markdown ### 3Ô∏è‚É£ Curate your images\n",
        "#@markdown We will find duplicate images with the FiftyOne AI, and mark them with `delete`. <p>\n",
        "#@markdown Then, an interactive area will appear below this cell that lets you visualize all your images and manually mark with `delete` to the ones you don't like. <p>\n",
        "#@markdown If the interactive area appears blank for over a minute, try enabling cookies and removing tracking protection for the Google Colab website, as they may break it.\n",
        "#@markdown Regardless, you can save your changes by sending Enter in the input box above the interactive area.<p>\n",
        "#@markdown This is how similar 2 images must be to be marked for deletion. I recommend 0.97 to 0.99:\n",
        "similarity_threshold = 0.985 #@param {type:\"number\"}\n",
        "\n",
        "\n",
        "os.chdir(root_dir)\n",
        "model_name = \"clip-vit-base32-torch\"\n",
        "supported_types = (\".png\", \".jpg\", \".jpeg\")\n",
        "img_count = len(os.listdir(images_folder))\n",
        "batch_size = min(250, img_count)\n",
        "\n",
        "if \"step3_installed_flag\" not in globals():\n",
        "  print(\"üè≠ Installing dependencies...\\n\")\n",
        "  !pip -q install fiftyone ftfy\n",
        "  !pip -q install fiftyone-db-ubuntu2204\n",
        "  if not get_ipython().__dict__['user_ns']['_exit_code']:\n",
        "    clear_output()\n",
        "    step3_installed_flag = True\n",
        "  else:\n",
        "    print(\"‚ùå Error installing dependencies, attempting to continue anyway...\")\n",
        "\n",
        "import numpy as np\n",
        "import fiftyone as fo\n",
        "import fiftyone.zoo as foz\n",
        "from fiftyone import ViewField as F\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "non_images = [f for f in os.listdir(images_folder) if not f.lower().endswith(supported_types)]\n",
        "if non_images:\n",
        "  print(f\"üí• Error: Found non-image file {non_images[0]} - This program doesn't allow it. Sorry! Use the Extras at the bottom to clean the folder.\")\n",
        "elif img_count == 0:\n",
        "  print(f\"üí• Error: No images found in {images_folder}\")\n",
        "else:\n",
        "  print(\"\\nüíø Analyzing dataset...\\n\")\n",
        "  dataset = fo.Dataset.from_dir(images_folder, dataset_type=fo.types.ImageDirectory)\n",
        "  model = foz.load_zoo_model(model_name)\n",
        "  embeddings = dataset.compute_embeddings(model, batch_size=batch_size)\n",
        "\n",
        "  batch_embeddings = np.array_split(embeddings, batch_size)\n",
        "  similarity_matrices = []\n",
        "  max_size_x = max(array.shape[0] for array in batch_embeddings)\n",
        "  max_size_y = max(array.shape[1] for array in batch_embeddings)\n",
        "\n",
        "  for i, batch_embedding in enumerate(batch_embeddings):\n",
        "    similarity = cosine_similarity(batch_embedding)\n",
        "    #Pad 0 for np.concatenate\n",
        "    padded_array = np.zeros((max_size_x, max_size_y))\n",
        "    padded_array[0:similarity.shape[0], 0:similarity.shape[1]] = similarity\n",
        "    similarity_matrices.append(padded_array)\n",
        "\n",
        "  similarity_matrix = np.concatenate(similarity_matrices, axis=0)\n",
        "  similarity_matrix = similarity_matrix[0:embeddings.shape[0], 0:embeddings.shape[0]]\n",
        "\n",
        "  similarity_matrix = cosine_similarity(embeddings)\n",
        "  similarity_matrix -= np.identity(len(similarity_matrix))\n",
        "\n",
        "  dataset.match(F(\"max_similarity\") > similarity_threshold)\n",
        "  dataset.tags = [\"delete\", \"has_duplicates\"]\n",
        "\n",
        "  id_map = [s.id for s in dataset.select_fields([\"id\"])]\n",
        "  samples_to_remove = set()\n",
        "  samples_to_keep = set()\n",
        "\n",
        "  for idx, sample in enumerate(dataset):\n",
        "    if sample.id not in samples_to_remove:\n",
        "      # Keep the first instance of two duplicates\n",
        "      samples_to_keep.add(sample.id)\n",
        "\n",
        "      dup_idxs = np.where(similarity_matrix[idx] > similarity_threshold)[0]\n",
        "      for dup in dup_idxs:\n",
        "          # We kept the first instance so remove all other duplicates\n",
        "          samples_to_remove.add(id_map[dup])\n",
        "\n",
        "      if len(dup_idxs) > 0:\n",
        "          sample.tags.append(\"has_duplicates\")\n",
        "          sample.save()\n",
        "    else:\n",
        "      sample.tags.append(\"delete\")\n",
        "      sample.save()\n",
        "\n",
        "  clear_output()\n",
        "\n",
        "  sidebar_groups = fo.DatasetAppConfig.default_sidebar_groups(dataset)\n",
        "  for group in sidebar_groups[1:]:\n",
        "    group.expanded = False\n",
        "  dataset.app_config.sidebar_groups = sidebar_groups\n",
        "  dataset.save()\n",
        "  session = fo.launch_app(dataset)\n",
        "\n",
        "  print(\"‚ùó Wait a minute for the session to load. If it doesn't, read above.\")\n",
        "  print(\"‚ùó When it's ready, you'll see a grid of your images.\")\n",
        "  print(\"‚ùó On the left side enable \\\"sample tags\\\" to visualize the images marked for deletion.\")\n",
        "  print(\"‚ùó You can mark your own images with the \\\"delete\\\" label by selecting them and pressing the tag icon at the top.\")\n",
        "  input(\"‚≠ï When you're done, enter something here to save your changes: \")\n",
        "\n",
        "  print(\"üíæ Saving...\")\n",
        "\n",
        "  marked = [s for s in dataset if \"delete\" in s.tags]\n",
        "  dataset.remove_samples(marked)\n",
        "  previous_folder = images_folder[:images_folder.rfind(\"/\")]\n",
        "  dataset.export(export_dir=os.path.join(images_folder, project_subfolder), dataset_type=fo.types.ImageDirectory)\n",
        "\n",
        "  temp_suffix = \"_temp\"\n",
        "  !mv {images_folder} {images_folder}{temp_suffix}\n",
        "  !mv {images_folder}{temp_suffix}/{project_subfolder} {images_folder}\n",
        "  !rm -r {images_folder}{temp_suffix}\n",
        "\n",
        "  session.refresh()\n",
        "  fo.close_app()\n",
        "  clear_output()\n",
        "\n",
        "  print(f\"\\n‚úÖ Removed {len(marked)} images from dataset. You now have {len(os.listdir(images_folder))} images.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_z5U2VVJ0mCr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y tensorflow ml-dtypes jax jaxlib optax orbax-checkpoint tensorstore\n",
        "!pip install tensorflow==2.17.0 jax==0.4.27 jaxlib==0.4.27 ml-dtypes==0.3.1 optax==0.2.3 orbax-checkpoint tensorstore==0.1.65"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "keTrWJ3v40Qn",
        "outputId": "aab56d12-2082-4e22-cc2d-66b5594c591e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping tensorflow as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping ml-dtypes as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping jax as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping jaxlib as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping optax as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping orbax-checkpoint as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping tensorstore as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting tensorflow==2.17.0\n",
            "  Using cached tensorflow-2.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Collecting jax==0.4.27\n",
            "  Using cached jax-0.4.27-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting jaxlib==0.4.27\n",
            "  Using cached jaxlib-0.4.27-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting ml-dtypes==0.3.1\n",
            "  Using cached ml_dtypes-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Collecting optax==0.2.3\n",
            "  Using cached optax-0.2.3-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting orbax-checkpoint\n",
            "  Using cached orbax_checkpoint-0.6.4-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting tensorstore==0.1.65\n",
            "  Using cached tensorstore-0.1.65-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (4.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (1.14.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (1.64.1)\n",
            "Collecting tensorboard<2.18,>=2.17 (from tensorflow==2.17.0)\n",
            "  Using cached tensorboard-2.17.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting keras>=3.2.0 (from tensorflow==2.17.0)\n",
            "  Using cached keras-3.5.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from jax==0.4.27) (1.13.1)\n",
            "Requirement already satisfied: chex>=0.1.86 in /usr/local/lib/python3.10/dist-packages (from optax==0.2.3) (0.1.86)\n",
            "Requirement already satisfied: etils[epy] in /usr/local/lib/python3.10/dist-packages (from optax==0.2.3) (1.9.4)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint) (1.0.8)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint) (6.0.2)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint) (1.6.0)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint) (4.10.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.17.0) (0.44.0)\n",
            "Collecting numpy<2.0.0,>=1.23.5 (from tensorflow==2.17.0)\n",
            "  Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.86->optax==0.2.3) (0.12.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow==2.17.0) (13.8.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow==2.17.0) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow==2.17.0) (0.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.17.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.17.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.17.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.17.0) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow==2.17.0) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow==2.17.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow==2.17.0) (3.0.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint) (2024.6.1)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint) (6.4.5)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint) (3.20.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow==2.17.0) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow==2.17.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow==2.17.0) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow==2.17.0) (0.1.2)\n",
            "Using cached tensorflow-2.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (601.3 MB)\n",
            "Using cached jax-0.4.27-py3-none-any.whl (1.9 MB)\n",
            "Using cached jaxlib-0.4.27-cp310-cp310-manylinux2014_x86_64.whl (77.3 MB)\n",
            "Downloading ml_dtypes-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (206 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m206.7/206.7 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached optax-0.2.3-py3-none-any.whl (289 kB)\n",
            "Using cached tensorstore-0.1.65-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.9 MB)\n",
            "Using cached orbax_checkpoint-0.6.4-py3-none-any.whl (270 kB)\n",
            "Using cached keras-3.5.0-py3-none-any.whl (1.1 MB)\n",
            "Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "Using cached tensorboard-2.17.1-py3-none-any.whl (5.5 MB)\n",
            "Installing collected packages: numpy, tensorboard, ml-dtypes, tensorstore, keras, jaxlib, jax, tensorflow, orbax-checkpoint, optax\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.23.5\n",
            "    Uninstalling numpy-1.23.5:\n",
            "      Successfully uninstalled numpy-1.23.5\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.15.2\n",
            "    Uninstalling tensorboard-2.15.2:\n",
            "      Successfully uninstalled tensorboard-2.15.2\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.15.0\n",
            "    Uninstalling keras-2.15.0:\n",
            "      Successfully uninstalled keras-2.15.0\n",
            "Successfully installed jax-0.4.27 jaxlib-0.4.27 keras-3.5.0 ml-dtypes-0.3.1 numpy-1.26.4 optax-0.2.3 orbax-checkpoint-0.6.4 tensorboard-2.17.1 tensorflow-2.17.0 tensorstore-0.1.65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  os.environ['TF_CPP_MIN_LOG_LEVEL'] = '0'\n",
        "  %env PYTHONPATH={kohya}\n",
        "  !python {kohya}/finetune/tag_images_by_wd14_tagger.py \\\n",
        "    {images_folder} \\\n",
        "    --repo_id=SmilingWolf/wd-v1-4-swinv2-tagger-v2 \\\n",
        "    --model_dir={root_dir} \\\n",
        "    --thresh={tag_threshold} \\\n",
        "    --batch_size=8 \\\n",
        "    --caption_extension=.txt \\\n",
        "    --force_download"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DCznJ0D5UbI",
        "outputId": "2e51be01-03c3-4553-9a0b-14a70f4b500f"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: PYTHONPATH=/content/kohya-trainer\n",
            "2024-09-28 09:33:08.445119: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-09-28 09:33:08.466547: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-09-28 09:33:08.472852: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-09-28 09:33:08.488506: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-09-28 09:33:09.539564: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "downloading wd14 tagger model from hf_hub. id: SmilingWolf/wd-v1-4-swinv2-tagger-v2\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1143: FutureWarning: The `force_filename` parameter is deprecated as a new caching system, which keeps the filenames as they are on the Hub, is now in place.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'cached_download' (from 'huggingface_hub.file_download') is deprecated and will be removed from version '0.26'. Use `hf_hub_download` instead.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "keras_metadata.pb: 100% 448k/448k [00:00<00:00, 14.2MB/s]\n",
            "saved_model.pb: 100% 37.6M/37.6M [00:00<00:00, 40.8MB/s]\n",
            "selected_tags.csv: 100% 254k/254k [00:00<00:00, 1.54MB/s]\n",
            "variables.data-00000-of-00001: 100% 385M/385M [00:01<00:00, 201MB/s]\n",
            "variables.index: 100% 24.2k/24.2k [00:00<00:00, 40.0MB/s]\n",
            "/content\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1727516001.980324   19644 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "I0000 00:00:1727516001.982400   19644 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "I0000 00:00:1727516001.982636   19644 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "I0000 00:00:1727516001.983309   19644 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "I0000 00:00:1727516001.983538   19644 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "I0000 00:00:1727516001.983720   19644 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "I0000 00:00:1727516002.090146   19644 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "I0000 00:00:1727516002.090481   19644 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-09-28 09:33:22.090617: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "I0000 00:00:1727516002.090732   19644 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-09-28 09:33:22.090881: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13949 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "found 15 images.\n",
            " 33% 5/15 [00:00<00:00, 49.21it/s]2024-09-28 09:33:45.202607: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 8902\n",
            "W0000 00:00:1727516025.359818   19718 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1727516025.482646   19718 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1727516025.484739   19718 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1727516025.486683   19718 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1727516025.488699   19718 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1727516025.490802   19718 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1727516025.494670   19718 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1727516025.496907   19718 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1727516025.499388   19718 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1727516025.501282   19718 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1727516025.503503   19718 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1727516025.515219   19718 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1727516025.517718   19718 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1727516025.522634   19718 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1727516025.524840   19718 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1727516025.529537   19718 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1727516025.531716   19718 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1727516025.533723   19718 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1727516025.536414   19718 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "tf.Tensor(\n",
            "[[8.3154514e-03 9.7706419e-01 3.9753951e-03 ... 1.3367985e-07\n",
            "  9.2522832e-08 6.8032598e-08]\n",
            " [6.4724588e-01 3.9297840e-01 3.5130797e-04 ... 6.6948473e-08\n",
            "  3.1569238e-07 2.1591021e-07]\n",
            " [1.7934880e-01 8.1636840e-01 3.2868484e-04 ... 5.6010145e-07\n",
            "  7.8113653e-06 5.4996181e-06]\n",
            " ...\n",
            " [5.2822500e-01 5.0204247e-01 7.8935886e-04 ... 3.3591050e-07\n",
            "  3.0679945e-07 1.5658985e-07]\n",
            " [6.0847634e-01 3.6135820e-01 7.0589012e-04 ... 5.8292211e-09\n",
            "  4.7902444e-07 2.8415147e-07]\n",
            " [7.6688498e-01 2.7623680e-01 8.4344158e-04 ... 3.4562910e-07\n",
            "  1.3196532e-06 5.2726119e-07]], shape=(8, 9083), dtype=float32)\n",
            "100% 15/15 [00:12<00:00,  1.22it/s]\n",
            "W0000 00:00:1727516033.704124   19717 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1727516033.705852   19717 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1727516033.707668   19717 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1727516033.709378   19717 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1727516033.711185   19717 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1727516033.713089   19717 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1727516033.714991   19717 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1727516033.717043   19717 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1727516033.719326   19717 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1727516033.721014   19717 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1727516033.723055   19717 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1727516033.725366   19717 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1727516033.727611   19717 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1727516033.729902   19717 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1727516033.731871   19717 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1727516033.734178   19717 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1727516033.736120   19717 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1727516033.737952   19717 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1727516033.740324   19717 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "tf.Tensor(\n",
            "[[6.9316888e-01 3.3042571e-01 4.2474043e-04 ... 1.1651304e-07\n",
            "  2.9712265e-07 1.5585059e-07]\n",
            " [7.8831953e-01 2.5996196e-01 6.8098027e-04 ... 8.2602241e-08\n",
            "  4.8721574e-07 4.2033562e-07]\n",
            " [7.9767984e-01 2.0428041e-01 1.3065764e-03 ... 4.5835947e-07\n",
            "  3.6551802e-07 3.2661248e-07]\n",
            " ...\n",
            " [7.7831078e-01 2.1635188e-01 4.1206449e-04 ... 2.1138643e-08\n",
            "  2.2633260e-07 2.0490432e-07]\n",
            " [7.7331716e-01 2.7839005e-01 4.1776194e-04 ... 3.7907835e-08\n",
            "  2.2634815e-07 1.7116741e-07]\n",
            " [4.0420424e-02 9.5421761e-01 2.0151725e-03 ... 2.7825362e-08\n",
            "  6.2619371e-08 4.0732434e-08]], shape=(7, 9083), dtype=float32)\n",
            "done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  if not get_ipython().__dict__['user_ns']['_exit_code']:\n",
        "    print(\"removing underscores and blacklist...\")\n",
        "    blacklisted_tags = [t.strip() for t in blacklist_tags.split(\",\")]\n",
        "    from collections import Counter\n",
        "    top_tags = Counter()\n",
        "    for txt in [f for f in os.listdir(images_folder) if f.lower().endswith(\".txt\")]:\n",
        "      with open(os.path.join(images_folder, txt), 'r') as f:\n",
        "        tags = [t.strip() for t in f.read().split(\",\")]\n",
        "        tags = [t.replace(\"_\", \" \") if len(t) > 3 else t for t in tags]\n",
        "        tags = [t for t in tags if t not in blacklisted_tags]\n",
        "      top_tags.update(tags)\n",
        "      with open(os.path.join(images_folder, txt), 'w') as f:\n",
        "        f.write(\", \".join(tags))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_oJRHkYs-0nL",
        "outputId": "3ebced98-6811-43af-e37d-08d614eef8ae"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "removing underscores and blacklist...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sl4FD7Mz-uea"
      },
      "outputs": [],
      "source": [
        "if \"step1_installed_flag\" not in globals():\n",
        "  raise Exception(\"Please run step 1 first!\")\n",
        "\n",
        "#@markdown ### 4Ô∏è‚É£ Tag your images\n",
        "#@markdown We will be using AI to automatically tag your images, specifically [Waifu Diffusion](https://huggingface.co/SmilingWolf/wd-v1-4-swinv2-tagger-v2) in the case of anime and [BLIP](https://huggingface.co/spaces/Salesforce/BLIP) in the case of photos.\n",
        "#@markdown Giving tags/captions to your images allows for much better training. This process should take a couple minutes. <p>\n",
        "method = \"Anime tags\" #@param [\"Anime tags\", \"Photo captions\"]\n",
        "#@markdown **Anime:** The threshold is the minimum level of confidence the tagger must have in order to include a tag. Lower threshold = More tags. Recommended 0.35 to 0.5\n",
        "tag_threshold = 0.35 #@param {type:\"slider\", min:0.0, max:1.0, step:0.01}\n",
        "blacklist_tags = \"bangs, breasts, multicolored hair, two-tone hair, gradient hair, virtual youtuber, parody, style parody, official alternate costume, official alternate hairstyle, official alternate hair length, alternate costume, alternate hairstyle, alternate hair length, alternate hair color\" #@param {type:\"string\"}\n",
        "#@markdown **Photos:** The minimum and maximum length of tokens/words in each caption.\n",
        "caption_min = 10 #@param {type:\"number\"}\n",
        "caption_max = 75 #@param {type:\"number\"}\n",
        "\n",
        "%env PYTHONPATH=/env/python\n",
        "os.chdir(root_dir)\n",
        "kohya = \"/content/kohya-trainer\"\n",
        "if not os.path.exists(kohya):\n",
        "  !git clone https://github.com/kohya-ss/sd-scripts {kohya}\n",
        "  os.chdir(kohya)\n",
        "  !git reset --hard 9a67e0df390033a89f17e70df5131393692c2a55\n",
        "  os.chdir(root_dir)\n",
        "\n",
        "if \"tags\" in method:\n",
        "  if \"step4a_installed_flag\" in globals():\n",
        "    del globals()['step4a_installed_flag']\n",
        "  if \"step4a_installed_flag\" not in globals():\n",
        "    print(\"\\nüè≠ Installing dependencies...\\n\")\n",
        "    !pip install accelerate diffusers einops==0.6.0 tensorflow==2.17.0 transformers safetensors huggingface-hub torchvision albumentations jax==0.4.27 jaxlib==0.4.27\n",
        "    if not get_ipython().__dict__['user_ns']['_exit_code']:\n",
        "      #clear_output()\n",
        "      step4a_installed_flag = True\n",
        "    else:\n",
        "      print(\"‚ùå Error installing dependencies, trying to continue anyway...\")\n",
        "\n",
        "  print(\"\\nüö∂‚Äç‚ôÇÔ∏è Launching program...\\n\")\n",
        "\n",
        "  os.environ['TF_CPP_MIN_LOG_LEVEL'] = '0'\n",
        "  %env PYTHONPATH={kohya}\n",
        "  !python {kohya}/finetune/tag_images_by_wd14_tagger.py \\\n",
        "    {images_folder} \\\n",
        "    --repo_id=SmilingWolf/wd-v1-4-swinv2-tagger-v2 \\\n",
        "    --model_dir={root_dir} \\\n",
        "    --thresh={tag_threshold} \\\n",
        "    --batch_size=8 \\\n",
        "    --caption_extension=.txt \\\n",
        "    --force_download\n",
        "\n",
        "  if not get_ipython().__dict__['user_ns']['_exit_code']:\n",
        "    print(\"removing underscores and blacklist...\")\n",
        "    blacklisted_tags = [t.strip() for t in blacklist_tags.split(\",\")]\n",
        "    from collections import Counter\n",
        "    top_tags = Counter()\n",
        "    for txt in [f for f in os.listdir(images_folder) if f.lower().endswith(\".txt\")]:\n",
        "      with open(os.path.join(images_folder, txt), 'r') as f:\n",
        "        tags = [t.strip() for t in f.read().split(\",\")]\n",
        "        tags = [t.replace(\"_\", \" \") if len(t) > 3 else t for t in tags]\n",
        "        tags = [t for t in tags if t not in blacklisted_tags]\n",
        "      top_tags.update(tags)\n",
        "      with open(os.path.join(images_folder, txt), 'w') as f:\n",
        "        f.write(\", \".join(tags))\n",
        "\n",
        "    %env PYTHONPATH=/env/python\n",
        "    clear_output()\n",
        "    print(f\"üìä Tagging complete. Here are the top 50 tags in your dataset:\")\n",
        "    print(\"\\n\".join(f\"{k} ({v})\" for k, v in top_tags.most_common(50)))\n",
        "\n",
        "\n",
        "else: # Photos\n",
        "  if \"step4b_installed_flag\" not in globals():\n",
        "    print(\"\\nüè≠ Installing dependencies...\\n\")\n",
        "    !pip install timm==0.6.12 fairscale==0.4.13 transformers==4.26.0 requests==2.28.2 accelerate==0.15.0 diffusers[torch]==0.10.2 einops==0.6.0 safetensors==0.2.6 jax==0.4.23 jaxlib==0.4.23\n",
        "    if not get_ipython().__dict__['user_ns']['_exit_code']:\n",
        "      clear_output()\n",
        "      step4b_installed_flag = True\n",
        "    else:\n",
        "      print(\"‚ùå Error installing dependencies, trying to continue anyway...\")\n",
        "\n",
        "  print(\"\\nüö∂‚Äç‚ôÇÔ∏è Launching program...\\n\")\n",
        "\n",
        "  os.chdir(kohya)\n",
        "  %env PYTHONPATH={kohya}\n",
        "  !python {kohya}/finetune/make_captions.py \\\n",
        "    {images_folder} \\\n",
        "    --beam_search \\\n",
        "    --max_data_loader_n_workers=2 \\\n",
        "    --batch_size=8 \\\n",
        "    --min_length={caption_min} \\\n",
        "    --max_length={caption_max} \\\n",
        "    --caption_extension=.txt\n",
        "\n",
        "  if not get_ipython().__dict__['user_ns']['_exit_code']:\n",
        "    import random\n",
        "    captions = [f for f in os.listdir(images_folder) if f.lower().endswith(\".txt\")]\n",
        "    sample = []\n",
        "    for txt in random.sample(captions, min(10, len(captions))):\n",
        "      with open(os.path.join(images_folder, txt), 'r') as f:\n",
        "        sample.append(f.read())\n",
        "\n",
        "    os.chdir(root_dir)\n",
        "    %env PYTHONPATH=/env/python\n",
        "    clear_output()\n",
        "    print(f\"üìä Captioning complete. Here are {len(sample)} example captions from your dataset:\")\n",
        "    print(\"\".join(sample))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "WBFik7accyDz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "482ebea0-a8d2-4471-8e1e-db1f07d2a566"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìé Applied new activation tag(s): fcc_lydia\n",
            "\n",
            "‚úÖ Done! Check your updated tags in the Extras below.\n"
          ]
        }
      ],
      "source": [
        "if \"step1_installed_flag\" not in globals():\n",
        "  raise Exception(\"Please run step 1 first!\")\n",
        "\n",
        "#@markdown ### 5Ô∏è‚É£ Curate your tags\n",
        "#@markdown Modify your dataset's tags. You can run this cell multiple times with different parameters. <p>\n",
        "\n",
        "#@markdown Put an activation tag at the start of every text file. This is useful to make learning better and activate your Lora easier. Set `keep_tokens` to 1 when training.<p>\n",
        "#@markdown Common tags that are removed such as hair color, etc. will be \"absorbed\" by your activation tag.\n",
        "global_activation_tag = \"fcc_lydia\" #@param {type:\"string\"}\n",
        "remove_tags = \"\" #@param {type:\"string\"}\n",
        "#@markdown &nbsp;\n",
        "\n",
        "#@markdown In this advanced section, you can search text files containing matching tags, and replace them with less/more/different tags. If you select the checkbox below, any extra tags will be put at the start of the file, letting you assign different activation tags to different parts of your dataset. Still, you may want a more advanced tool for this.\n",
        "search_tags = \"\" #@param {type:\"string\"}\n",
        "replace_with = \"\" #@param {type:\"string\"}\n",
        "search_mode = \"OR\" #@param [\"OR\", \"AND\"]\n",
        "new_becomes_activation_tag = False #@param {type:\"boolean\"}\n",
        "#@markdown These may be useful sometimes. Will remove existing activation tags, be careful.\n",
        "sort_alphabetically = False #@param {type:\"boolean\"}\n",
        "remove_duplicates = False #@param {type:\"boolean\"}\n",
        "\n",
        "def split_tags(tagstr):\n",
        "  return [s.strip() for s in tagstr.split(\",\") if s.strip()]\n",
        "\n",
        "activation_tag_list = split_tags(global_activation_tag)\n",
        "remove_tags_list = split_tags(remove_tags)\n",
        "search_tags_list = split_tags(search_tags)\n",
        "replace_with_list = split_tags(replace_with)\n",
        "replace_new_list = [t for t in replace_with_list if t not in search_tags_list]\n",
        "\n",
        "replace_with_list = [t for t in replace_with_list if t not in replace_new_list]\n",
        "replace_new_list.reverse()\n",
        "activation_tag_list.reverse()\n",
        "\n",
        "remove_count = 0\n",
        "replace_count = 0\n",
        "\n",
        "for txt in [f for f in os.listdir(images_folder) if f.lower().endswith(\".txt\")]:\n",
        "\n",
        "  with open(os.path.join(images_folder, txt), 'r') as f:\n",
        "    tags = [s.strip() for s in f.read().split(\",\")]\n",
        "\n",
        "  if remove_duplicates:\n",
        "    tags = list(set(tags))\n",
        "  if sort_alphabetically:\n",
        "    tags.sort()\n",
        "\n",
        "  for rem in remove_tags_list:\n",
        "    if rem in tags:\n",
        "      remove_count += 1\n",
        "      tags.remove(rem)\n",
        "\n",
        "  if \"AND\" in search_mode and all(r in tags for r in search_tags_list) \\\n",
        "      or \"OR\" in search_mode and any(r in tags for r in search_tags_list):\n",
        "    replace_count += 1\n",
        "    for rem in search_tags_list:\n",
        "      if rem in tags:\n",
        "        tags.remove(rem)\n",
        "    for add in replace_with_list:\n",
        "      if add not in tags:\n",
        "        tags.append(add)\n",
        "    for new in replace_new_list:\n",
        "      if new_becomes_activation_tag:\n",
        "        if new in tags:\n",
        "          tags.remove(new)\n",
        "        tags.insert(0, new)\n",
        "      else:\n",
        "        if new not in tags:\n",
        "          tags.append(new)\n",
        "\n",
        "  for act in activation_tag_list:\n",
        "    if act in tags:\n",
        "      tags.remove(act)\n",
        "    tags.insert(0, act)\n",
        "\n",
        "  with open(os.path.join(images_folder, txt), 'w') as f:\n",
        "    f.write(\", \".join(tags))\n",
        "\n",
        "if global_activation_tag:\n",
        "  print(f\"\\nüìé Applied new activation tag(s): {', '.join(activation_tag_list)}\")\n",
        "if remove_tags:\n",
        "  print(f\"\\nüöÆ Removed {remove_count} tags.\")\n",
        "if search_tags:\n",
        "  print(f\"\\nüí´ Replaced in {replace_count} files.\")\n",
        "print(\"\\n‚úÖ Done! Check your updated tags in the Extras below.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "HuJB7BGAyZCw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 58
        },
        "outputId": "18f11ee1-df37-41bf-9d82-b9652ce11472"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### ü¶Ä [Click here to open the Lora trainer](https://colab.research.google.com/github/hollowstrawberry/kohya-colab/blob/main/Lora_Trainer.ipynb)"
          },
          "metadata": {}
        }
      ],
      "source": [
        "#@markdown ### 6Ô∏è‚É£ Ready\n",
        "#@markdown You should be ready to [train your Lora](https://colab.research.google.com/github/hollowstrawberry/kohya-colab/blob/main/Lora_Trainer.ipynb)!\n",
        "\n",
        "from IPython.display import Markdown, display\n",
        "display(Markdown(f\"### ü¶Ä [Click here to open the Lora trainer](https://colab.research.google.com/github/hollowstrawberry/kohya-colab/blob/main/Lora_Trainer.ipynb)\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDB9GXRONfiU"
      },
      "source": [
        "## *Ô∏è‚É£ Extras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "cellView": "form",
        "id": "xEsqOglcc6hA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afe38351-a576-47a7-a4bd-6c457df501aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Top 50 tags:\n",
            "looking at viewer (15)\n",
            "1girl (14)\n",
            "solo (14)\n",
            "shirt (14)\n",
            "blush (12)\n",
            "braid (11)\n",
            "outdoors (11)\n",
            "long hair (10)\n",
            "standing (10)\n",
            "day (10)\n",
            "blonde hair (9)\n",
            "green eyes (9)\n",
            "long sleeves (9)\n",
            "white shirt (9)\n",
            "building (9)\n",
            "skirt (8)\n",
            "closed mouth (8)\n",
            "holding (7)\n",
            "black skirt (7)\n",
            "bag (7)\n",
            "smile (7)\n",
            "road (7)\n",
            "tree (7)\n",
            "hair between eyes (6)\n",
            "city (6)\n",
            "bare shoulders (5)\n",
            "cowboy shot (5)\n",
            "bottle (5)\n",
            "blue eyes (5)\n",
            "pleated skirt (5)\n",
            "street (5)\n",
            "brown hair (5)\n",
            "hair ornament (4)\n",
            "indoors (4)\n",
            "sky (4)\n",
            "hair intakes (4)\n",
            "shoulder bag (4)\n",
            "plant (4)\n",
            "short hair (4)\n",
            "dress (3)\n",
            "parted lips (3)\n",
            "sleeveless (3)\n",
            "black dress (3)\n",
            "window (3)\n",
            "hair over shoulder (3)\n",
            "collared shirt (3)\n",
            "belt (3)\n",
            "blue sky (3)\n",
            "thighhighs (3)\n",
            "detached sleeves (3)\n"
          ]
        }
      ],
      "source": [
        "if \"step1_installed_flag\" not in globals():\n",
        "  raise Exception(\"Please run step 1 first!\")\n",
        "\n",
        "#@markdown ### üìà Analyze Tags\n",
        "#@markdown Perhaps you need another look at your dataset.\n",
        "show_top_tags = 50 #@param {type:\"number\"}\n",
        "\n",
        "from collections import Counter\n",
        "top_tags = Counter()\n",
        "\n",
        "for txt in [f for f in os.listdir(images_folder) if f.lower().endswith(\".txt\")]:\n",
        "  with open(os.path.join(images_folder, txt), 'r') as f:\n",
        "    top_tags.update([s.strip() for s in f.read().split(\",\")])\n",
        "\n",
        "top_tags = Counter(top_tags)\n",
        "print(f\"üìä Top {show_top_tags} tags:\")\n",
        "for k, v in top_tags.most_common(show_top_tags):\n",
        "  print(f\"{k} ({v})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "x56xQYwuOz2V"
      },
      "outputs": [],
      "source": [
        "#@markdown ### üìÇ Unzip dataset\n",
        "#@markdown It's much slower to upload individual files to your Drive, so you may want to upload a zip if you have your dataset in your computer.\n",
        "zip = \"/content/drive/MyDrive/Loras/example.zip\" #@param {type:\"string\"}\n",
        "extract_to = \"/content/drive/MyDrive/Loras/example/dataset\" #@param {type:\"string\"}\n",
        "\n",
        "import os, zipfile\n",
        "\n",
        "if not os.path.exists('/content/drive'):\n",
        "  from google.colab import drive\n",
        "  print(\"üìÇ Connecting to Google Drive...\")\n",
        "  drive.mount('/content/drive')\n",
        "\n",
        "os.makedirs(extract_to, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(zip, 'r') as f:\n",
        "  f.extractall(extract_to)\n",
        "\n",
        "print(\"‚úÖ Done\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "dLetTcLVOvAE"
      },
      "outputs": [],
      "source": [
        "#@markdown ### üî¢ Count datasets\n",
        "#@markdown Google Drive makes it impossible to count the files in a folder, so this will show you the file counts in all folders and subfolders.\n",
        "folder = \"/content/drive/MyDrive/Loras\" #@param {type:\"string\"}\n",
        "\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "if not os.path.exists('/content/drive'):\n",
        "    print(\"üìÇ Connecting to Google Drive...\\n\")\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "tree = {}\n",
        "exclude = (\"_logs\", \"/output\")\n",
        "for i, (root, dirs, files) in enumerate(os.walk(folder, topdown=True)):\n",
        "  dirs[:] = [d for d in dirs if all(ex not in d for ex in exclude)]\n",
        "  images = len([f for f in files if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))])\n",
        "  captions = len([f for f in files if f.lower().endswith(\".txt\")])\n",
        "  others = len(files) - images - captions\n",
        "  path = root[folder.rfind(\"/\")+1:]\n",
        "  tree[path] = None if not images else f\"{images:>4} images | {captions:>4} captions |\"\n",
        "  if tree[path] and others:\n",
        "    tree[path] += f\" {others:>4} other files\"\n",
        "\n",
        "pad = max(len(k) for k in tree)\n",
        "print(\"\\n\".join(f\"üìÅ{k.ljust(pad)} | {v}\" for k, v in tree.items() if v))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AeR9EI4AhT2p"
      },
      "outputs": [],
      "source": [
        "if \"step1_installed_flag\" not in globals():\n",
        "  raise Exception(\"Please run step 1 first!\")\n",
        "\n",
        "from PIL import Image\n",
        "import os\n",
        "Image.MAX_IMAGE_PIXELS = None\n",
        "\n",
        "#@markdown ### üñºÔ∏è Reduce dataset filesize\n",
        "#@markdown This will convert all images in the project folder to jpeg, reducing filesize without affecting quality too much. This can also solve some errors.\n",
        "location = images_folder\n",
        "\n",
        "for dir in [d[0] for d in os.walk(location)]:\n",
        "    os.chdir(dir)\n",
        "    converted = False\n",
        "    for file_name in list(os.listdir(\".\")):\n",
        "        try:\n",
        "            # Convert png to jpeg\n",
        "            if file_name.endswith(\".png\"):\n",
        "                if not converted:\n",
        "                    print(f\"Converting {dir}\")\n",
        "                    converted = True\n",
        "                im = Image.open(file_name)\n",
        "                im = im.convert(\"RGB\")\n",
        "                new_file_name = os.path.splitext(file_name)[0] + \".jpeg\"\n",
        "                im.save(new_file_name, quality=95)\n",
        "                os.remove(file_name)\n",
        "                file_name = new_file_name\n",
        "            # Resize large jpegs\n",
        "            if file_name.endswith((\".jpeg\", \".jpg\")) and os.path.getsize(file_name) > 2000000:\n",
        "                if not converted:\n",
        "                    print(f\"Converting {dir}\")\n",
        "                    converted = True\n",
        "                im = Image.open(file_name)\n",
        "                im = im.resize((int(im.width/2), int(im.height/2)))\n",
        "                im.save(file_name, quality=95)\n",
        "            # Rename jpg to jpeg\n",
        "            if file_name.endswith(\".jpg\"):\n",
        "                if not converted:\n",
        "                    print(f\"Converting {dir}\")\n",
        "                new_file_name = os.path.splitext(file_name)[0] + \".jpeg\"\n",
        "                os.rename(file_name, new_file_name)\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred while processing {file_name}: {e}\")\n",
        "    if converted:\n",
        "        print(f\"Converted {dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "y6PKW-LIr214"
      },
      "outputs": [],
      "source": [
        "if \"step1_installed_flag\" not in globals():\n",
        "  raise Exception(\"Please run step 1 first!\")\n",
        "\n",
        "#@markdown ### üöÆ Clean folder\n",
        "#@markdown Careful! Deletes all non-image files in the project folder.\n",
        "\n",
        "!find {images_folder} -type f ! \\( -name '*.png' -o -name '*.jpg' -o -name '*.jpeg' \\) -delete\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}